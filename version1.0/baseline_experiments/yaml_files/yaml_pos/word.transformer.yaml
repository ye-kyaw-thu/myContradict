# word.transformer.yaml

## Where the samples will be written
save_data: ./word_transformer_final2

# Prevent overwriting existing files in the folder
overwrite: True

## Where the vocab(s) will be written
src_vocab: ./word_transformer_final2/word_transformer.vocab.src
tgt_vocab: ./word_transformer_final2/word_transformer.vocab.tgt
vocab_size_multiple: 8
src_words_min_frequency: 1
tgt_words_min_frequency: 1
share_vocab: True
n_sample: 0

#### Filter
src_seq_length: 200
tgt_seq_length: 200

# Corpus opts:
data:
    train:
        path_src: ./parallel_pseudo/sT/myPOS_transformer_word_pos.train.src.txt
        path_tgt: ./parallel_pseudo/sT/myPOS_transformer_word_pos.train.tgt.txt
        transforms: [inferfeats, filtertoolong]
        weight: 1
        
    valid:
        path_src: ./parallel_pseudo/w_pos/word_valid.src.txt.TAGGED.TAGGED
        path_tgt: ./parallel_pseudo/original/word_valid.tgt.txt
        transforms: [inferfeats]     

# Features options
n_src_feats: 1
feat_merge: "mlp"
feat_vec_size: 512
src_feats_defaults: "x"

# Transform options
reversible_tokenization: "joiner"

# Model Configuration

save_model: ./word_transformer_final2/word_transformer_final2
log_file: ./word_transformer_final2/log_final.txt

# Stop training if it does not improve after n validations
early_stopping: 10

# Default: 5000 - Save a model checkpoint for each n
# save_checkpoint_steps: 1000

decoder_type: transformer
encoder_type: transformer
word_vec_size: 512
hidden_size: 512
enc_layers: 6
dec_layers: 6
transformer_ff: 2048
heads: 8

accum_count: 4

# Optimization
model_dtype: "fp16"
optim: adam
adam_beta1: 0.9
adam_beta2: 0.998
decay_method: noam
learning_rate: 0.1
max_grad_norm: 0.0

batch_size: 64
batch_type: tokens
normalization: tokens
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]
position_encoding: true
label_smoothing: 0.1

max_generator_batches: 2

param_init: 0.0
param_init_glorot: 'true'
position_encoding: 'true'

world_size: 1
## Run with GPU no. zero
gpu_ranks: [0]

# Number of training iterations
train_steps: 30000

