# word.transformer.yaml

## Where the samples will be written
save_data: ./word_transformer

# Prevent overwriting existing files in the folder
overwrite: True

## Where the vocab(s) will be written
src_vocab: ./word_transformer/word_transformer.vocab.src
tgt_vocab: ./word_transformer/word_transformer.vocab.tgt
vocab_size_multiple: 8
src_words_min_frequency: 1
tgt_words_min_frequency: 1
share_vocab: True
n_sample: 0

#### Filter
src_seq_length: 200
tgt_seq_length: 200

# Corpus opts:
data:
    train:
        path_src: ./parallel_pseudo/sT/myPOS_transformer_word.train.src.txt
        path_tgt: ./parallel_pseudo/sT/myPOS_transformer_word.train.tgt.txt
    valid:
        path_src: ./parallel_pseudo/original/word_valid.src.txt
        path_tgt: ./parallel_pseudo/original/word_valid.tgt.txt

# Model Configuration

save_model: ./word_transformer/word_transformer
log_file: ./word_transformer/log_3k_word_transformer_ST.txt

# Stop training if it does not imporve after n validations
early_stopping: 10

# Default: 5000 - Save a model checkpoint for each n
# save_checkpoint_steps: 1000

decoder_type: transformer
encoder_type: transformer
word_vec_size: 512
hidden_size: 512
enc_layers: 6
dec_layers: 6
hidden_size: 512
word_vec_size: 512
transformer_ff: 512
heads: 8

accum_count: 4

# Optimization
learning_rate: 0.1

batch_size: 64
batch_type: tokens
normalization: tokens
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]
position_encoding: true
label_smoothing: 0.1

max_generator_batches: 2

param_init: 0.0
param_init_glorot: 'true'
position_encoding: 'true'

world_size: 1
## Run with GPU no. zero
gpu_ranks: [0]

# Number of training iterations
train_steps: 30000
