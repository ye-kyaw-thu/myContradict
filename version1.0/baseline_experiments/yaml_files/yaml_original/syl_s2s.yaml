# syl_s2s.yaml
## Where the samples will be written

# Vocab Building
## Where the samples will be written
save_data: ./s2s_syl

## Where the vocab(s) will be written
src_vocab: ./s2s_syl/syl_s2s.vocab.src
tgt_vocab: ./s2s_syl/syl_s2s.vocab.tgt
share_vocab: True

# Prevent overwriting existing files in the folder
overwrite: True

# Corpus opts:
data:
    train:
        path_src: ./parallel_pseudo/sT/s2s_syl.src.txt
        path_tgt: ./parallel_pseudo/sT/s2s_syl.tgt.txt
    valid:
        path_src: ./parallel_pseudo/original/syl_valid.src.txt
        path_tgt: ./parallel_pseudo/original/syl_valid.tgt.txt

# Increase sequence length
src_seq_length: 200  
tgt_seq_length: 200  

# Training

save_model: ./s2s_syl/s2s_syl

log_file: ./s2s_syl/log_3k_s2s_syl_ST.txt

# Stop training if it does not imporve after n validations
early_stopping: 10

# Default: 5000 - Save a model checkpoint for each n
# save_checkpoint_steps: 10000

# To save space, limit checkpoints to last n
keep_checkpoint: 5

# Optimization
learning_rate: 0.1

batch_size: 64
dropout: [0.3]  # LSTM models often use higher dropout rates

# Train on a single GPU
world_size: 1
gpu_ranks: [0]

# Number of training iterations
train_steps: 30000

# Model parameters for RNN LSTM
encoder_type: brnn
decoder_type: rnn
rnn_type: LSTM
rnn_size: 512  # Adjust this based on your model size and complexity
rnn_num_layers: 2  # Adjust this based on model architecture
